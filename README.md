# Weighted-L2-Divergence
Detecting Local Closeness with Weighted L2 Divergence
![Contour](https://github.com/dem123456789/Weighted-L2-Divergence/blob/master/result/figures/mixture/contour.jpg "Contour") 
![P-value](https://github.com/dem123456789/Weighted-L2-Divergence/blob/master/result/figures/p_value.jpg "P-value") 
 
## Abstract

In this paper, we propose a novel method to measure the local closeness of two unknown distributions, using
observed data only. To that end, we propose a nonparametric estimator of a distance between two density functions,
which is in the form of L2
2 distance weighted by a kernel function. Such design aims to amplify the difference
between distributions by focusing on a specific data domain. The proposed distance and its estimator can also
be applied when a data analyst has prior on where in the data domain to focus on, and (passively) measure the
corresponding differences. It can also be applied to test the equality of distributions, with a power of statistical
hypothesis test better than that of some popular methods. The idea is to apply a pre-screening step to (actively)
choose the optimal weight functions, and then test. It is especially helpful when a data analyst has no prior knowledge
about the underlying distributions. Rigorous theoretical analysis is provided that guarantees asymptotic performance.

## Contributing

1. Fork
2. Create your feature branch: `git checkout -b my-new-feature`
3. Commit your changes: `git commit -am 'Add some feature'`
4. Push to the branch: `git push origin my-new-feature`
5. Submit a pull request

## Source

*Detecting Local Closeness with Weighted L 2 2 Divergence*,  
Deng Z, Ding J, Diao E, et al.  

## Credits

*Zhun Deng  
Enmao Diao     
Jie Ding  
Vahid Tarokh*

